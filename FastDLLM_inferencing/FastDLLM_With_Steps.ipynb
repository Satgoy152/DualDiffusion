{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd073f9-dfc6-4dc2-a141-a797e5943c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 20:42:32.963723: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-28 20:42:33.026560: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-28 20:42:34.756455: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146986b773774a0c814e5309d276fd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, GenerationConfig\n",
    "from Fast_dLLM_v2_7B.modeling import Fast_dLLM_QwenForCausalLM\n",
    "\n",
    "model_name = \"Efficient-Large-Model/Fast_dLLM_7B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# remote config (no remote code execution)\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# using local class to load remote weights\n",
    "model = Fast_dLLM_QwenForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    config=config, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",)  # downloads weights from Hub\n",
    "\n",
    "# (optional) generation parameters from the repo\n",
    "gen_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config = gen_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29715afe-920d-429f-b6f2-323306395487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a type of artificial intelligence model designed to process and generate human language.|<MASK>| are typically trained on massive|<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>|\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "gen_text = \"A large language model is\"\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "text = text + gen_text\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Fast-dLLM v2 parallel decoding\n",
    "gen_ids, past_key_values, past_block_key_values = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    small_block_size=8,\n",
    "    threshold=0.95,\n",
    "    steps=18,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(\n",
    "    gen_ids[0][inputs[\"input_ids\"].shape[1]:], \n",
    "    skip_special_tokens=False\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd7bb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A large language model\n",
      "tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198,  35127,    752,    264,   2805,\n",
      "          16800,    311,   3460,   4128,   1614,     13, 151645,    198, 151644,\n",
      "          77091,    198,     32,   3460,   4128,   1614,    374]])\n",
      "tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198,  35127,    752,    264,   2805,\n",
      "          16800,    311,   3460,   4128,   1614,     13, 151645,    198, 151644,\n",
      "          77091,    198,     32,   3460,   4128,   1614,    374,    264,    943,\n",
      "            315,  20443,  11229,   1614,   6188,    311,   1882,    323,   6923,\n",
      "           3738,   4128,     13, 151665,    525,  11136,  16176,    389,  10951,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665]])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(inputs[\"input_ids\"][0][:-1], skip_special_tokens=False))\n",
    "print(inputs[\"input_ids\"])\n",
    "print(gen_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62455d6f-1e76-454f-8e71-5649690919b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198,  35127,    752,    264,   2805,\n",
      "          16800,    311,   3460,   4128,   1614,     13, 151645,    198, 151644,\n",
      "          77091,    198,     32,   3460,   4128,   1614,    374,    264,    943,\n",
      "            315,  20443,  11229,   1614,   6188,    311,   1882,    323,   6923,\n",
      "           3738,   4128,     13]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A large language model is a type of artificial intelligence model designed to process and generate human language. They are typically trained on massive datasets of text, allowing them to learn intricate patterns,|<MASK>|,|<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>|\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "mask_id = 151665\n",
    "indices = torch.where(gen_ids[0] == mask_id)[0]\n",
    "\n",
    "stripped_ids = gen_ids[0][:indices[0]].unsqueeze(0)\n",
    "\n",
    "print(stripped_ids)\n",
    "\n",
    "gen_ids, past_key_values, past_block_key_values = model.generate(\n",
    "    stripped_ids,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=236,\n",
    "    small_block_size=8,\n",
    "    threshold=0.95,\n",
    "    steps=18,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(\n",
    "    gen_ids[0], \n",
    "    skip_special_tokens=False\n",
    ")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee593972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer])\n"
     ]
    }
   ],
   "source": [
    "print(past_key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "462eed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.1367, -0.0767, -0.2402,  ...,  0.2637,  0.1406,  0.0684],\n",
      "          [ 0.1592,  0.1182, -0.6016,  ...,  0.1128,  0.0608,  0.2617],\n",
      "          [-0.0250,  0.1152, -0.0649,  ...,  0.0173,  0.0334, -0.0238],\n",
      "          ...,\n",
      "          [ 0.2949, -0.0040,  0.4004,  ...,  0.3223,  0.0403, -0.1074],\n",
      "          [-0.0086,  0.1011, -0.0386,  ...,  0.1973,  0.0403, -0.3711],\n",
      "          [ 0.1709, -0.1074,  0.1084,  ..., -0.2988,  0.4375, -0.0654]],\n",
      "\n",
      "         [[-0.0723,  0.0366,  0.7109,  ...,  0.1533,  0.0664,  0.0942],\n",
      "          [-0.0576,  0.0352,  1.0391,  ...,  0.4570, -0.3672,  0.3691],\n",
      "          [ 0.1426, -0.1553, -0.3711,  ...,  0.1025,  0.0209,  0.0283],\n",
      "          ...,\n",
      "          [ 0.5547, -0.0376,  0.7305,  ..., -0.6602,  0.0277, -0.0500],\n",
      "          [-0.5586,  0.0515,  1.4297,  ...,  0.0503,  0.3047, -0.4199],\n",
      "          [ 0.0664,  0.1855,  0.5938,  ...,  0.4648,  0.0035, -0.7305]],\n",
      "\n",
      "         [[-0.1147, -0.0498, -0.1758,  ...,  0.1260, -0.2305, -0.3535],\n",
      "          [ 0.1602,  0.1670,  0.0391,  ...,  0.1436,  0.0723,  0.0825],\n",
      "          [-0.0806,  0.0708,  0.0713,  ...,  0.2910,  0.1338,  0.0635],\n",
      "          ...,\n",
      "          [-0.2617, -0.1973,  0.0933,  ..., -0.2500, -0.2178, -0.2832],\n",
      "          [-0.3691,  0.1060, -0.4062,  ..., -0.0469, -0.1660,  0.0684],\n",
      "          [-0.5156,  0.1494, -0.1177,  ...,  0.1641, -0.0166,  0.2383]],\n",
      "\n",
      "         [[ 0.0664,  0.2812, -0.0238,  ..., -0.1064, -0.1079,  0.0508],\n",
      "          [ 0.1021,  0.0957,  0.1152,  ..., -0.2236,  0.2715, -0.0210],\n",
      "          [ 0.0066, -0.0371,  0.0081,  ...,  0.0057, -0.0576, -0.0026],\n",
      "          ...,\n",
      "          [-0.2178, -0.0903,  0.2236,  ...,  0.2910,  0.0757,  0.3711],\n",
      "          [ 0.6367, -0.4375,  0.2256,  ...,  0.2656,  0.3242, -0.1953],\n",
      "          [-0.3320, -0.4043, -0.0170,  ..., -0.0581,  0.4590,  0.2266]]]],\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(past_key_values.layers[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "786965c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DynamicLayer, DynamicCache\n",
    "\n",
    "def truncate_dynamic_cache(dynamic_cache, new_len):\n",
    "    \"\"\"\n",
    "    Truncates a HuggingFace DynamicCache to match a shortened sequence length.\n",
    "    \"\"\"\n",
    "    for i in range(len(dynamic_cache)):\n",
    "        if dynamic_cache.layers[i].keys is not None:\n",
    "            dynamic_cache.layers[i].keys = dynamic_cache.layers[i].keys[:, :, :new_len, :].contiguous()\n",
    "        if dynamic_cache.layers[i].values is not None:\n",
    "            dynamic_cache.layers[i].values = dynamic_cache.layers[i].values[:, :, :new_len, :].contiguous()\n",
    "    return dynamic_cache\n",
    "\n",
    "def truncate_block_cache(block_cache, new_len, block_size):\n",
    "    max_blocks = (new_len + block_size - 1) // block_size\n",
    "    return block_cache[:max_blocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4c9d9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer])\n"
     ]
    }
   ],
   "source": [
    "new_seq_len = stripped_ids[0].size(0)\n",
    "print(new_seq_len)\n",
    "\n",
    "past_key_values_trunc = truncate_dynamic_cache(past_key_values, new_seq_len)\n",
    "print(past_key_values_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30622410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198,  35127,    752,    264,   2805,\n",
      "          16800,    311,   3460,   4128,   1614,     13, 151645,    198, 151644,\n",
      "          77091,    198,     32,   3460,   4128,   1614,    374,    264,    943,\n",
      "            315,  20443,  11229,   1614,   6188,    311,   1882,    323,   6923,\n",
      "           3738,   4128,     13,   2379,    525,  11136,  16176,    389,  10951,\n",
      "          29425,    315,   1467,     11,  10693,   1105,    311,   3960,  56116,\n",
      "          12624,     11,  14389,     11,    323,  11871,   2878,   4128,     13,\n",
      "           4220,   4119,    646,   2736,   5257,   5810,   4128,   8692,   9079,\n",
      "             11,   1741,    438]])\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A large language model is a type of artificial intelligence model designed to process and generate human language. They are typically trained on massive datasets of text, allowing them to learn intricate patterns, structures, and relationships within language. These models can perform various natural language processing tasks, such as language translation, text summarization, and question-answering, with remarkable accuracy and coherence.|<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>|\n"
     ]
    }
   ],
   "source": [
    "mask_id = 151665\n",
    "indices = torch.where(gen_ids[0] == mask_id)[0]\n",
    "\n",
    "stripped_ids = gen_ids[0][:indices[0]].unsqueeze(0)\n",
    "\n",
    "print(stripped_ids)\n",
    "\n",
    "gen_ids, past_key_values, past_block_key_values = model.generate(\n",
    "    stripped_ids,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=236,\n",
    "    small_block_size=8,\n",
    "    threshold=0.95,\n",
    "    steps=18,\n",
    "    past_key_values=past_key_values_trunc\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(\n",
    "    gen_ids[0], \n",
    "    skip_special_tokens=False\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a2144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
