{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d9aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4242bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 11:56:28.834082: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-27 11:56:30.249732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-27 11:56:33.514264: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317f86c639b143b2ba1530e22525669e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9310fb89719c4b8daf79bafe73a8009b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_llada.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct:\n",
      "- configuration_llada.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe678bc7beb4c43b0f5eedbd382d675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_llada.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct:\n",
      "- modeling_llada.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13c6e4abdc64271a243fb927e79c20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96aca110956481398528cf97ea49120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c93ed0f6124c0da8d08c71f8d62d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c05ebb04dd4defa2751ead631f7934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54331a61d5a849abaf064eebeaac81ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/2.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e715636d0924eab8d27c995b419fce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a88228045ea4728bba82c159dbe7ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edbe479df13469c8c038f695a144ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/2.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0b8f7e0b8d414b9a321f92a93cd57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7d702c763d473e917638be3e8cc36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21a19673094495ab553025f2a6c1d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61392e4414e549b8a8fc95272817e31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e7f93bd7be44eaa7062e086d19e29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/747 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5535c446acb4bc0ab50170fa3882149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoConfig, GenerationConfig\n",
    "from FastDLLM_inferencing.Fast_dLLM_v2_7B.modeling import Fast_dLLM_QwenForCausalLM\n",
    "\n",
    "\n",
    "# load LLaDa\n",
    "device = 'cuda'\n",
    "verifier = AutoModel.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True, dtype=torch.bfloat16)\n",
    "verifier_tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True)\n",
    "\n",
    "\n",
    "# load fast dLLM\n",
    "model_name = \"Efficient-Large-Model/Fast_dLLM_7B\"\n",
    "\n",
    "drafter_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# remote config (no remote code execution)\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# using local class to load remote weights\n",
    "drafter = Fast_dLLM_QwenForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    config=config, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",)  # downloads weights from Hub\n",
    "\n",
    "# (optional) generation parameters from the repo\n",
    "gen_config = GenerationConfig.from_pretrained(model_name)\n",
    "drafter.generation_config = gen_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c4ce94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fast_dLLM_QwenForCausalLM(\n",
       "  (model): Fast_dLLM_QwenModel(\n",
       "    (embed_tokens): Embedding(152064, 3584, padding_idx=151645)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Fast_dLLM_QwenDecoderLayer(\n",
       "        (self_attn): Fast_dLLM_QwenAttention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Fast_dLLM_QwenMLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Fast_dLLM_QwenRMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Fast_dLLM_QwenRMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Fast_dLLM_QwenRMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Fast_dLLM_QwenRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure everything is on device\n",
    "verifier.to(device)\n",
    "drafter.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19ec03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdllm_inf(prompt, tokenizer, model):\n",
    "    \n",
    "    prompt = \"Give me a short introduction to large language model.\"\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\"role\": \"assistant\", \"content\": \"A large language model (LLM) is a type of artificial intelligence model designed to process and generate human\"}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    gen_ids, past_key_values, past_block_key_values = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    small_block_size=8,\n",
    "    threshold=0.95,\n",
    "    steps=16,\n",
    "    )\n",
    "\n",
    "    # response = tokenizer.decode(\n",
    "    #     gen_ids[0][inputs[\"input_ids\"].shape[1]:], \n",
    "    #     skip_special_tokens=False\n",
    "    # )\n",
    "    # print(response)\n",
    "\n",
    "    # response = tokenizer.decode(\n",
    "    #     gen_ids[0][inputs[\"input_ids\"].shape[1]:], \n",
    "    #     skip_special_tokens=False\n",
    "    # )\n",
    "\n",
    "    return gen_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a451cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLaDA.generate import generate, generate_per_step\n",
    "\n",
    "# llada inference\n",
    "def llada_inf(model, tokenizer, context_tensor):\n",
    "    gen_length = 256\n",
    "\n",
    "    # m = [{\"role\": \"user\", \"content\": query}]\n",
    "    # user_input = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
    "    # input_ids = tokenizer(user_input)['input_ids']\n",
    "    input_ids = torch.tensor(context_tensor).to(device)\n",
    "    # query = input_ids\n",
    "\n",
    "    out = generate_per_step(model, \n",
    "        input_ids, \n",
    "        n = 1, \n",
    "        k = 1,\n",
    "        gen_length=gen_length, \n",
    "        block_length=gen_length, \n",
    "        temperature=0.0, \n",
    "        remasking='low_confidence')\n",
    "\n",
    "    answer = tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n",
    "    print(answer)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58275fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198,  35127,    752,    264,   2805,\n",
      "          16800,    311,   3460,   4128,   1614,     13, 151645,    198, 151644,\n",
      "          77091,    198,     32,   3460,   4128,   1614,    320,   4086,     44,\n",
      "              8,    374,    264,    943,    315,  20443,  11229,   1614,   6188,\n",
      "            311,   1882,    323,   6923,   3738, 151645,    198,  11528,     13,\n",
      "           1084,    525,  11136,  16176,    389,  10951,  14713,    315,   1467,\n",
      "            821,     11, 151665,   1105,    311,   3535, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665,\n",
      "         151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665, 151665]],\n",
      "       device='cuda:0') 1\n",
      "tensor([[    27,  18621,    198,   2496,    449,    259,   9031,  16841,     13,\n",
      "             27,    198,     27,   3840,    198,  29380,    696,    259,   3228,\n",
      "          14340,    297,   3193,   2295,   2718,     13,     27,    198,     27,\n",
      "            598,    198,     32,   3193,   2295,   2718,    363,   2477,     44,\n",
      "              8,    341,    259,   2023,    300,  20232,  14420,   2718,   5469,\n",
      "            297,   1829,    301,   8941,   3738,     27,    198,  18991,     13,\n",
      "            963,    449,   9507,  14421,    366,  12610,  14073,    300,   3019,\n",
      "           1354,     11, 126336,    989,    297,   2725, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336]],\n",
      "       device='cuda:0') 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2817512/4261871206.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(context_tensor).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-10 tokens per position at step 0:\n",
      "\n",
      "Position 133:\n",
      "  Rank   Token                          Probability \n",
      "  ------------------------------------------------\n",
      "  1      <|endoftext|>                  0.234375     ◄ SELECTED\n",
      "  2      ,                              0.057373     \n",
      "  3       and                           0.027954     \n",
      "  4      .                              0.027100     \n",
      "  5      \\n                             0.024658     \n",
      "  6       to                            0.016479     \n",
      "  7      Ms                             0.016479     \n",
      "  8       of                            0.014526     \n",
      "  9      <                              0.012817     \n",
      "  10      language                      0.012024     \n",
      "\n",
      "Position 134:\n",
      "  Rank   Token                          Probability \n",
      "  ------------------------------------------------\n",
      "  1      <|endoftext|>                  0.235352     ◄ SELECTED\n",
      "  2      ,                              0.059570     \n",
      "  3       and                           0.029907     \n",
      "  4      .                              0.027222     \n",
      "  5      \\n                             0.024780     \n",
      "  6      Ms                             0.017090     \n",
      "  7       to                            0.016479     \n",
      "  8       of                            0.015015     \n",
      "  9       language                      0.013245     \n",
      "  10     <                              0.011353     \n",
      "\n",
      "Position 135:\n",
      "  Rank   Token                          Probability \n",
      "  ------------------------------------------------\n",
      "  1      <|endoftext|>                  0.224609     ◄ SELECTED\n",
      "  2      ,                              0.062256     \n",
      "  3      .                              0.029419     \n",
      "  4       and                           0.028564     \n",
      "  5      \\n                             0.026855     \n",
      "  6      Ms                             0.018433     \n",
      "  7       to                            0.016724     \n",
      "  8       of                            0.015259     \n",
      "  9       language                      0.013489     \n",
      "  10     <                              0.013062     \n",
      "================================================================================\n",
      "\n",
      "['<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|endoftext|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>', '<|mdm_mask|>']\n",
      "<system\n",
      "You are a helpful assistant.<\n",
      "<user\n",
      "Give me a short introduction to large language model.<\n",
      "<ass\n",
      "A large language model (LLM) is a type of artificial intelligence model designed to process and generate human<\n",
      "language. It are typically trained on massive amounts of text data, them to understand\n",
      "tensor([[    27,  18621,    198,   2496,    449,    259,   9031,  16841,     13,\n",
      "             27,    198,     27,   3840,    198,  29380,    696,    259,   3228,\n",
      "          14340,    297,   3193,   2295,   2718,     13,     27,    198,     27,\n",
      "            598,    198,     32,   3193,   2295,   2718,    363,   2477,     44,\n",
      "              8,    341,    259,   2023,    300,  20232,  14420,   2718,   5469,\n",
      "            297,   1829,    301,   8941,   3738,     27,    198,  18991,     13,\n",
      "            963,    449,   9507,  14421,    366,  12610,  14073,    300,   3019,\n",
      "           1354,     11, 126336,    989,    297,   2725, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336,\n",
      "         126336, 126336, 126336, 126336, 126336, 126336, 126336, 126336, 126081,\n",
      "         126336, 126336, 126336, 126336]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from inference import convert\n",
    "\n",
    "drafted = fdllm_inf(\"Hi what can you do?\", drafter_tokenizer, drafter)\n",
    "print(drafted, len(drafted))\n",
    "\n",
    "drafted_retok = convert(151665,126336, drafted, drafter_tokenizer, verifier_tokenizer)\n",
    "print(drafted_retok, len(drafted_retok))\n",
    "\n",
    "verified = llada_inf(verifier, verifier_tokenizer, drafted_retok)\n",
    "print(verified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b42a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
