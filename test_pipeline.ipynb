{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2edd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f00fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 18:51:57.185904: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-30 18:51:57.247339: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-30 18:51:59.240738: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807e7fa9d94749edb70e16e9f11d5caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261cda68158b4792b1cd6e28f50e4685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoConfig, GenerationConfig\n",
    "from FastDLLM_inferencing.Fast_dLLM_v2_7B.modeling import Fast_dLLM_QwenForCausalLM\n",
    "\n",
    "\n",
    "# load LLaDa\n",
    "device = 'cuda'\n",
    "verifier = AutoModel.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True, dtype=torch.bfloat16)\n",
    "verifier_tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True)\n",
    "\n",
    "\n",
    "# load fast dLLM\n",
    "model_name = \"Efficient-Large-Model/Fast_dLLM_7B\"\n",
    "\n",
    "drafter_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# remote config (no remote code execution)\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# using local class to load remote weights\n",
    "drafter = Fast_dLLM_QwenForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    config=config, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",)  # downloads weights from Hub\n",
    "\n",
    "# (optional) generation parameters from the repo\n",
    "gen_config = GenerationConfig.from_pretrained(model_name)\n",
    "drafter.generation_config = gen_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4fea71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fast_dLLM_QwenForCausalLM(\n",
       "  (model): Fast_dLLM_QwenModel(\n",
       "    (embed_tokens): Embedding(152064, 3584, padding_idx=151645)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Fast_dLLM_QwenDecoderLayer(\n",
       "        (self_attn): Fast_dLLM_QwenAttention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Fast_dLLM_QwenMLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Fast_dLLM_QwenRMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Fast_dLLM_QwenRMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Fast_dLLM_QwenRMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Fast_dLLM_QwenRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure everything is on device\n",
    "verifier.to(device)\n",
    "drafter.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f29a6849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model wrapper functions for Fast_dLLM and LLaDA\n",
    "# Add this to your notebook or create a new file: model_wrappers.py\n",
    "\n",
    "import torch\n",
    "from LLaDA.generate import generate_per_step\n",
    "\n",
    "\n",
    "def fastdllm_generate_fn(model, tokenizer, input_ids, num_steps, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper for Fast_dLLM generation to match pipeline interface.\n",
    "    \n",
    "    Args:\n",
    "        model: Fast_dLLM model\n",
    "        tokenizer: Fast_dLLM tokenizer\n",
    "        input_ids: Input tensor\n",
    "        num_steps: Number of steps\n",
    "        **kwargs: Additional arguments (small_block_size, threshold, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        Generated tensor\n",
    "    \"\"\"\n",
    "    # Extract Fast_dLLM specific parameters or use defaults\n",
    "    small_block_size = kwargs.get('small_block_size', 8)\n",
    "    threshold = kwargs.get('threshold', 0.95)\n",
    "    max_new_tokens = kwargs.get('max_new_tokens', 256)\n",
    "    mask_id = kwargs.get('mask_id', 151665)\n",
    "    mask_positions = (input_ids == mask_id)\n",
    "    if mask_positions.any():\n",
    "        # Find first masked position\n",
    "        first_mask_idx = torch.where(mask_positions[0])[0][0].item()\n",
    "        prompt = input_ids[:, :first_mask_idx]\n",
    "    else:\n",
    "        # No masks, use entire input as prompt\n",
    "        prompt = input_ids\n",
    "\n",
    "    prompt_len = len(prompt)\n",
    "    \n",
    "    # Fast_dLLM returns (gen_ids, past_key_values, past_block_key_values)\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        block_size=32,\n",
    "        small_block_size=small_block_size,\n",
    "        threshold=threshold,\n",
    "        steps=num_steps,\n",
    "    )\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def llada_generate_fn(model, tokenizer, input_ids, num_steps, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper for LLaDA generation to match pipeline interface.\n",
    "    \n",
    "    Args:\n",
    "        model: LLaDA model\n",
    "        tokenizer: LLaDA tokenizer\n",
    "        input_ids: Input tensor (should already have prompt + masked tokens)\n",
    "        num_steps: Number of steps (n)\n",
    "        **kwargs: Additional arguments (k, gen_length, block_length, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        Generated tensor\n",
    "    \"\"\"\n",
    "    # Extract LLaDA specific parameters\n",
    "    k = kwargs.get('k', 1)  # tokens per step\n",
    "    gen_length = kwargs.get('gen_length', 256)\n",
    "    block_length = kwargs.get('block_length', 256)\n",
    "    temperature = kwargs.get('temperature', 0.0)\n",
    "    remasking = kwargs.get('remasking', 'low_confidence')\n",
    "    mask_id = kwargs.get('mask_id', 126336)\n",
    "    \n",
    "    # LLaDA's generate_per_step expects just the prompt, not prompt+masked\n",
    "    # So we need to extract the prompt part\n",
    "    # Assuming the masked tokens are at the end\n",
    "    mask_positions = (input_ids == mask_id)\n",
    "    if mask_positions.any():\n",
    "        # Find first masked position\n",
    "        first_mask_idx = torch.where(mask_positions[0])[0][0].item()\n",
    "        prompt = input_ids[:, :first_mask_idx]\n",
    "    else:\n",
    "        # No masks, use entire input as prompt\n",
    "        prompt = input_ids\n",
    "    \n",
    "    output = generate_per_step(\n",
    "        model,\n",
    "        prompt,\n",
    "        n=num_steps,\n",
    "        k=k,\n",
    "        gen_length=gen_length,\n",
    "        block_length=block_length,\n",
    "        temperature=temperature,\n",
    "        remasking=remasking,\n",
    "        mask_id=mask_id\n",
    "    )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9edf27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full decoded base prompt (with special tokens):\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language models.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "|<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>|\n",
      "###############################################\n",
      "Full decoded for drafter step 16 (with special tokens):\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language models.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Large language models (LLMs) are advanced artificial intelligence systems designed to|<MASK>| and generate human-like|<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>|\n",
      "###############################################\n",
      "Conversion: 227 tokens -> 227 tokens\n",
      "Full decoded for conversion (with special tokens):\n",
      "<|startoftext|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Give me a short introduction to large language models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Large language models (LLMs) are advanced artificial intelligence systems designed to<|mdm_mask|> and generate human-like<|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|>\n",
      "###############################################\n",
      "Full decoded for verfier step 1 (with special tokens):\n",
      "<|startoftext|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Give me a short introduction to large language models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Large language models (LLMs) are advanced artificial intelligence systems designed to<|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|endoftext|>\n",
      "Conversion: 270 tokens -> 269 tokens\n",
      "Warning: Length mismatch. Drafter: 256, Verifier(conv): 298. Truncating to 256.\n",
      "Drafter Logits:  torch.Size([1, 32, 152064])\n",
      "Verifier Logits:  torch.Size([1, 256, 126464])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 304 is out of bounds for dimension 1 with size 256",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mverification_algos\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confidence_threshold_verification\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m result = \u001b[43mdual_diffusion_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Models\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrafter_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrafter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrafter_tokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrafter_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverifier_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverifier_tokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverifier_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Input\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGive me a short introduction to large language models.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Steps\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_drafter_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_verifier_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Mask IDs\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrafter_mask_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m151665\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Fast_dLLM mask ID\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverifier_mask_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m126336\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# LLaDA mask ID\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Custom generate functions\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrafter_generate_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfastdllm_generate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverifier_generate_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllada_generate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Verification (None = use default trust_verifier)\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfidence_threshold_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Iteration control\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Single draft-verify pass\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Model-specific kwargs\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43msmall_block_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgen_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremasking\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlow_confidence\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     42\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerated text:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m'\u001b[39m\u001b[33moutput_text\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DualDiffusion/dual_pipeline.py:119\u001b[39m, in \u001b[36mdual_diffusion_generate\u001b[39m\u001b[34m(drafter_model, drafter_tokenizer, verifier_model, verifier_tokenizer, query, max_new_tokens, num_drafter_steps, num_verifier_steps, drafter_mask_id, verifier_mask_id, drafter_generate_fn, verifier_generate_fn, verification_fn, max_iterations, remask_threshold, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m decoded_full = drafter_tokenizer.decode(initial_input[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    118\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFull decoded base prompt (with special tokens):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28mprint\u001b[39m(decoded_full)\n\u001b[32m    120\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m###############################################\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# Start with just the prompt for the first generation\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: index 304 is out of bounds for dimension 1 with size 256"
     ]
    }
   ],
   "source": [
    "from dual_pipeline import dual_diffusion_generate\n",
    "from inference import convert\n",
    "from verification_algos import confidence_threshold_verification\n",
    "\n",
    "result = dual_diffusion_generate(\n",
    "    # Models\n",
    "    drafter_model=drafter,\n",
    "    drafter_tokenizer=drafter_tokenizer,\n",
    "    verifier_model=verifier,\n",
    "    verifier_tokenizer=verifier_tokenizer,\n",
    "    \n",
    "    # Input\n",
    "    query=\"Give me a short introduction to large language models.\",\n",
    "    max_new_tokens=256,\n",
    "    \n",
    "    # Steps\n",
    "    num_drafter_steps=16,\n",
    "    num_verifier_steps=1,\n",
    "    \n",
    "    # Mask IDs\n",
    "    drafter_mask_id=151665,  # Fast_dLLM mask ID\n",
    "    verifier_mask_id=126336,  # LLaDA mask ID\n",
    "    \n",
    "    # Custom generate functions\n",
    "    drafter_generate_fn=fastdllm_generate_fn,\n",
    "    verifier_generate_fn=llada_generate_fn,\n",
    "    \n",
    "    # Verification (None = use default trust_verifier)\n",
    "    verification_fn=confidence_threshold_verification,\n",
    "    \n",
    "    # Iteration control\n",
    "    max_iterations=4,  # Single draft-verify pass\n",
    "    \n",
    "    # Model-specific kwargs\n",
    "    small_block_size=8,\n",
    "    threshold=0.95,\n",
    "    k=1,\n",
    "    gen_length=256,\n",
    "    block_length=256,\n",
    "    temperature=0.0,\n",
    "    remasking='low_confidence'\n",
    ")\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(result['output_text'])\n",
    "print(\"\\nStats:\")\n",
    "print(result['stats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d210444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
