{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2edd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f00fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagoyal/research/DualDiffusion/.venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 149.35it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoConfig, GenerationConfig\n",
    "from FastDLLM_inferencing.Fast_dLLM_v2_7B.modeling import Fast_dLLM_QwenForCausalLM\n",
    "\n",
    "\n",
    "# load LLaDa\n",
    "device = 'cuda'\n",
    "verifier = AutoModel.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True, dtype=torch.bfloat16)\n",
    "verifier_tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True)\n",
    "\n",
    "\n",
    "# load fast dLLM\n",
    "model_name = \"Efficient-Large-Model/Fast_dLLM_7B\"\n",
    "\n",
    "drafter_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# remote config (no remote code execution)\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# using local class to load remote weights\n",
    "drafter = Fast_dLLM_QwenForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    config=config, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",)  # downloads weights from Hub\n",
    "\n",
    "# (optional) generation parameters from the repo\n",
    "gen_config = GenerationConfig.from_pretrained(model_name)\n",
    "drafter.generation_config = gen_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4fea71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fast_dLLM_QwenForCausalLM(\n",
       "  (model): Fast_dLLM_QwenModel(\n",
       "    (embed_tokens): Embedding(152064, 3584, padding_idx=151645)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Fast_dLLM_QwenDecoderLayer(\n",
       "        (self_attn): Fast_dLLM_QwenAttention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Fast_dLLM_QwenMLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Fast_dLLM_QwenRMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Fast_dLLM_QwenRMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Fast_dLLM_QwenRMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Fast_dLLM_QwenRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure everything is on device\n",
    "verifier.to(device)\n",
    "drafter.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29a6849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model wrapper functions for Fast_dLLM and LLaDA\n",
    "# Add this to your notebook or create a new file: model_wrappers.py\n",
    "\n",
    "import torch\n",
    "from LLaDA.generate import generate_per_step\n",
    "\n",
    "\n",
    "def fastdllm_generate_fn(model, tokenizer, input_ids, num_steps, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper for Fast_dLLM generation to match pipeline interface.\n",
    "    \n",
    "    Args:\n",
    "        model: Fast_dLLM model\n",
    "        tokenizer: Fast_dLLM tokenizer\n",
    "        input_ids: Input tensor\n",
    "        num_steps: Number of steps\n",
    "        **kwargs: Additional arguments (small_block_size, threshold, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        Generated tensor\n",
    "    \"\"\"\n",
    "    # Extract Fast_dLLM specific parameters or use defaults\n",
    "    small_block_size = kwargs.get('small_block_size', 8)\n",
    "    threshold = kwargs.get('threshold', 0.95)\n",
    "    max_new_tokens = kwargs.get('max_new_tokens', 256)\n",
    "    \n",
    "    # Fast_dLLM returns (gen_ids, past_key_values, past_block_key_values)\n",
    "    gen_ids, _, _ = model.generate(\n",
    "        input_ids,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        small_block_size=small_block_size,\n",
    "        threshold=threshold,\n",
    "        steps=num_steps,\n",
    "    )\n",
    "    \n",
    "    return gen_ids\n",
    "\n",
    "\n",
    "def llada_generate_fn(model, tokenizer, input_ids, num_steps, **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper for LLaDA generation to match pipeline interface.\n",
    "    \n",
    "    Args:\n",
    "        model: LLaDA model\n",
    "        tokenizer: LLaDA tokenizer\n",
    "        input_ids: Input tensor (should already have prompt + masked tokens)\n",
    "        num_steps: Number of steps (n)\n",
    "        **kwargs: Additional arguments (k, gen_length, block_length, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        Generated tensor\n",
    "    \"\"\"\n",
    "    # Extract LLaDA specific parameters\n",
    "    k = kwargs.get('k', 1)  # tokens per step\n",
    "    gen_length = kwargs.get('gen_length', 256)\n",
    "    block_length = kwargs.get('block_length', 256)\n",
    "    temperature = kwargs.get('temperature', 0.0)\n",
    "    remasking = kwargs.get('remasking', 'low_confidence')\n",
    "    mask_id = kwargs.get('mask_id', 126336)\n",
    "    \n",
    "    # LLaDA's generate_per_step expects just the prompt, not prompt+masked\n",
    "    # So we need to extract the prompt part\n",
    "    # Assuming the masked tokens are at the end\n",
    "    mask_positions = (input_ids == mask_id)\n",
    "    if mask_positions.any():\n",
    "        # Find first masked position\n",
    "        first_mask_idx = torch.where(mask_positions[0])[0][0].item()\n",
    "        prompt = input_ids[:, :first_mask_idx]\n",
    "    else:\n",
    "        # No masks, use entire input as prompt\n",
    "        prompt = input_ids\n",
    "    \n",
    "    output = generate_per_step(\n",
    "        model,\n",
    "        prompt,\n",
    "        n=num_steps,\n",
    "        k=k,\n",
    "        gen_length=gen_length,\n",
    "        block_length=block_length,\n",
    "        temperature=temperature,\n",
    "        remasking=remasking,\n",
    "        mask_id=mask_id\n",
    "    )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9edf27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full decoded base prompt (with special tokens):\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language models.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "|<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>|\n",
      "###############################################\n",
      "Full decoded for drafter step 16 (with special tokens):\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language models.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "|<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>|, including text, and audio, and can be used for various tasks, such|<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>|\n",
      "###############################################\n",
      "convert tokenizer 1 to text ['<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGive me a short introduction to large language models.<|im_end|>\\n<|im_start|>assistant\\n|<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>|, including text, and audio, and can be used for various tasks, such|<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>||<MASK>|']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [1, 512] at index 1 does not match the shape of the indexed tensor [1, 1931] at index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdual_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dual_diffusion_generate\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m result = \u001b[43mdual_diffusion_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Models\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrafter_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrafter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrafter_tokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrafter_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverifier_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverifier_tokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverifier_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Input\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGive me a short introduction to large language models.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Steps\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_drafter_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_verifier_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Mask IDs\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrafter_mask_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m151665\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Fast_dLLM mask ID\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverifier_mask_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m126336\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# LLaDA mask ID\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Custom generate functions\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrafter_generate_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfastdllm_generate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverifier_generate_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllada_generate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Verification (None = use default trust_verifier)\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Iteration control\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Single draft-verify pass\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Model-specific kwargs\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43msmall_block_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgen_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremasking\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlow_confidence\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     41\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerated text:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m'\u001b[39m\u001b[33moutput_text\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/DualDiffusion/dual_pipeline.py:142\u001b[39m, in \u001b[36mdual_diffusion_generate\u001b[39m\u001b[34m(drafter_model, drafter_tokenizer, verifier_model, verifier_tokenizer, query, max_new_tokens, num_drafter_steps, num_verifier_steps, drafter_mask_id, verifier_mask_id, drafter_generate_fn, verifier_generate_fn, verification_fn, max_iterations, remask_threshold, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m###############################################\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# 2b. Convert to verifier vocabulary\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m verifier_input = \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrafter_mask_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverifier_mask_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrafter_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrafter_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverifier_tokenizer\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m decoded_full = verifier_tokenizer.decode(verifier_input[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    151\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFull decoded for conversion (with special tokens):\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/DualDiffusion/inference.py:41\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(maskid1, maskid2, context_tensor, tokenizer1, tokenizer2)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Handle length mismatch - proportional masking\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# original_mask_ratio = mask_positions.float().mean()\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# num_masks = int(len(new_ids) * original_mask_ratio)\u001b[39;00m\n\u001b[32m     39\u001b[39m output_tensor = torch.tensor(new_ids, dtype=context_tensor.dtype, \n\u001b[32m     40\u001b[39m                               device=context_tensor.device)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43moutput_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_positions\u001b[49m\u001b[43m]\u001b[49m = maskid2\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Mask tokens uniformly\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# if num_masks > 0:\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m#     mask_indices = torch.linspace(0, len(new_ids)-1, num_masks).long()\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m#     output_tensor[mask_indices] = maskid2\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output_tensor\n",
      "\u001b[31mIndexError\u001b[39m: The shape of the mask [1, 512] at index 1 does not match the shape of the indexed tensor [1, 1931] at index 1"
     ]
    }
   ],
   "source": [
    "from dual_pipeline import dual_diffusion_generate\n",
    "from inference import convert\n",
    "\n",
    "result = dual_diffusion_generate(\n",
    "    # Models\n",
    "    drafter_model=drafter,\n",
    "    drafter_tokenizer=drafter_tokenizer,\n",
    "    verifier_model=verifier,\n",
    "    verifier_tokenizer=verifier_tokenizer,\n",
    "    \n",
    "    # Input\n",
    "    query=\"Give me a short introduction to large language models.\",\n",
    "    max_new_tokens=256,\n",
    "    \n",
    "    # Steps\n",
    "    num_drafter_steps=16,\n",
    "    num_verifier_steps=1,\n",
    "    \n",
    "    # Mask IDs\n",
    "    drafter_mask_id=151665,  # Fast_dLLM mask ID\n",
    "    verifier_mask_id=126336,  # LLaDA mask ID\n",
    "    \n",
    "    # Custom generate functions\n",
    "    drafter_generate_fn=fastdllm_generate_fn,\n",
    "    verifier_generate_fn=llada_generate_fn,\n",
    "    \n",
    "    # Verification (None = use default trust_verifier)\n",
    "    verification_fn=None,\n",
    "    \n",
    "    # Iteration control\n",
    "    max_iterations=1,  # Single draft-verify pass\n",
    "    \n",
    "    # Model-specific kwargs\n",
    "    small_block_size=8,\n",
    "    threshold=0.95,\n",
    "    k=1,\n",
    "    gen_length=256,\n",
    "    block_length=256,\n",
    "    temperature=0.0,\n",
    "    remasking='low_confidence'\n",
    ")\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(result['output_text'])\n",
    "print(\"\\nStats:\")\n",
    "print(result['stats'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
